{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "akKo1idoWRpT"
      },
      "source": [
        "# Simple Voice Chatbot with OpenAI API, Gradio and Whisper"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6YI1YGsRXvV_"
      },
      "source": [
        "## ðŸš€ Demo "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "og1A3RKpWn8B"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "l6REbRI-X2va"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "' # Gradio UI\\nwith gr.Blocks() as demo:\\n    chatbot = gr.Chatbot()\\n\\n    # Textbox for user input\\n    msg = gr.Textbox()\\n    # The output needs to be properly structured, returning valid Gradio components (chatbot and audio output)\\n    msg.submit(run_text_prompt, [msg, chatbot], [msg, chatbot])\\n\\n    # Row for audio input\\n    with gr.Row():\\n        audio = gr.Audio(type=\"filepath\")\\n        send_audio_button = gr.Button(\"Send Audio\", interactive=True)\\n        send_audio_button.click(run_audio_prompt, [audio, chatbot], [None, chatbot, None])\\n\\n    # Play the generated speech audio\\n    with gr.Row():\\n        audio_output = gr.Audio(label=\"Generated Audio\")\\n\\n    # Link audio generation output\\n    send_audio_button.click(fn=run_audio_prompt, inputs=[audio, chatbot], outputs=[None, chatbot, audio_output])\\n\\n# Launch the Gradio interface\\ndemo.launch(debug=True) '"
            ]
          },
          "execution_count": 64,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from pathlib import Path\n",
        "import gradio as gr\n",
        "from openai import OpenAI\n",
        "import assemblyai as aai\n",
        "from tempfile import NamedTemporaryFile\n",
        "import os\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "load_dotenv()\n",
        "\n",
        "# Initialize OpenAI and AssemblyAI APIs\n",
        "client = OpenAI()\n",
        "aai.settings.api_key = os.getenv(\"ASSEMBLYAI_API_KEY\")\n",
        "transcriber = aai.Transcriber()\n",
        "\n",
        "class Chat:\n",
        "    def __init__(self, system=None):\n",
        "        self.system = system\n",
        "        self.messages = []\n",
        "        \n",
        "        if system is not None:\n",
        "            self.messages.append({\n",
        "                \"role\": \"system\",\n",
        "                \"content\": system\n",
        "            })\n",
        "\n",
        "    def prompt(self, content: str) -> str:\n",
        "        self.messages.append({\n",
        "            \"role\": \"user\",\n",
        "            \"content\": content\n",
        "        })\n",
        "        response = client.chat.completions.create(\n",
        "            model=\"gpt-4o-mini\",\n",
        "            messages=self.messages\n",
        "        )\n",
        "        response_content = response.choices[0].message.content\n",
        "        self.messages.append({\n",
        "            \"role\": \"assistant\",\n",
        "            \"content\": response_content\n",
        "        })\n",
        "        return response_content\n",
        "\n",
        "chat = Chat(system=\"You are a helpful assistant.\")\n",
        "\n",
        "def run_text_prompt(message, chat_history):\n",
        "    # Get AI-generated response\n",
        "    bot_message = chat.prompt(content=message)\n",
        "\n",
        "    # Generate and save the voice response using OpenAI's TTS\n",
        "    speech_file_path = NamedTemporaryFile(suffix=\".mp3\", delete=False).name\n",
        "    \n",
        "    try:\n",
        "        response = client.audio.speech.create(\n",
        "            model=\"tts-1\",\n",
        "            voice=\"alloy\",\n",
        "            input=bot_message\n",
        "        )\n",
        "        if \"audio\" in response:\n",
        "            with open(speech_file_path, \"wb\") as audio_file:\n",
        "                audio_file.write(response[\"audio\"])\n",
        "        else:\n",
        "            print(\"Audio generation failed: \", response)\n",
        "            return \"\", chat_history, None\n",
        "    except Exception as e:\n",
        "        print(f\"Error generating audio: {e}\")\n",
        "        return \"\", chat_history, None\n",
        "\n",
        "    # Append the conversation history\n",
        "    chat_history.append((message, bot_message))\n",
        "\n",
        "    # Return the conversation history along with the audio path\n",
        "    return \"\", chat_history, speech_file_path\n",
        "\n",
        "def run_audio_prompt(audio, chat_history):\n",
        "    if audio is None:\n",
        "        return None, chat_history, None\n",
        "\n",
        "    # Transcribe audio using AssemblyAI\n",
        "    config = aai.TranscriptionConfig(speaker_labels=True)\n",
        "    transcript = transcriber.transcribe(audio, config)\n",
        "\n",
        "    if transcript.status == aai.TranscriptStatus.error:\n",
        "        return f\"Transcription failed: {transcript.error}\", chat_history, None\n",
        "    \n",
        "    message_transcription = transcript.text\n",
        "\n",
        "    # Run the text through the chatbot\n",
        "    _, chat_history, speech_file_path = run_text_prompt(message_transcription, chat_history)\n",
        "    \n",
        "    return None, chat_history, speech_file_path\n",
        "\n",
        "\"\"\" # Gradio UI\n",
        "with gr.Blocks() as demo:\n",
        "    chatbot = gr.Chatbot()\n",
        "\n",
        "    # Textbox for user input\n",
        "    msg = gr.Textbox()\n",
        "    # The output needs to be properly structured, returning valid Gradio components (chatbot and audio output)\n",
        "    msg.submit(run_text_prompt, [msg, chatbot], [msg, chatbot])\n",
        "\n",
        "    # Row for audio input\n",
        "    with gr.Row():\n",
        "        audio = gr.Audio(type=\"filepath\")\n",
        "        send_audio_button = gr.Button(\"Send Audio\", interactive=True)\n",
        "        send_audio_button.click(run_audio_prompt, [audio, chatbot], [None, chatbot, None])\n",
        "\n",
        "    # Play the generated speech audio\n",
        "    with gr.Row():\n",
        "        audio_output = gr.Audio(label=\"Generated Audio\")\n",
        "\n",
        "    # Link audio generation output\n",
        "    send_audio_button.click(fn=run_audio_prompt, inputs=[audio, chatbot], outputs=[None, chatbot, audio_output])\n",
        "\n",
        "# Launch the Gradio interface\n",
        "demo.launch(debug=True) \"\"\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'\\n\\n# Test AssemblyAI transcription\\ndef test_transcription(audio_file_path):\\n    config = aai.TranscriptionConfig(speaker_labels=True)\\n\\n    try:\\n        transcript = transcriber.transcribe(audio_file_path, config)\\n\\n        if transcript.status == aai.TranscriptStatus.completed:\\n            print(\"Transcription:\", transcript.text)\\n        else:\\n            print(\"Transcription failed:\", transcript.error)\\n    except Exception as e:\\n        print(f\"Error transcribing audio: {e}\")\\n\\n\\n# Run tests\\nif __name__ == \"__main__\":\\n    print(\"Testing Chat functionality:\")\\n    #test_chat()\\n\\n    print(\"\\nTesting OpenAI TTS:\")\\n    test_tts()\\n\\n    print(\"\\nTesting AssemblyAI Transcription:\")\\n    # Ensure you provide a valid audio file path here for testing transcription\\n    test_audio_file_path = \"test.mp3\"  # Replace with actual path\\n    test_transcription(test_audio_file_path)\\n '"
            ]
          },
          "execution_count": 73,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from openai import OpenAI\n",
        "import assemblyai as aai\n",
        "from tempfile import NamedTemporaryFile\n",
        "import os\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "# Load API keys from environment variables\n",
        "load_dotenv()\n",
        "aai.settings.api_key = os.getenv(\"ASSEMBLYAI_API_KEY\")\n",
        "transcriber = aai.Transcriber()\n",
        "\n",
        "\n",
        "# Test the Chat class\n",
        "def test_chat():\n",
        "    chat = Chat(system=\"You are a helpful assistant.\")\n",
        "    user_input = \"What's the weather like today?\"\n",
        "    response = chat.prompt(content=user_input)\n",
        "    print(\"Chat response:\", response)\n",
        "\n",
        "\n",
        "# Test OpenAI TTS\n",
        "def test_tts(test_text):\n",
        "    #test_text = \"Saucotec is gonna take over the world.\"\n",
        "    speech_file_path = \"test.mp3\"  # NamedTemporaryFile(suffix=\".mp3\", delete=False).name\n",
        "\n",
        "    try:\n",
        "        response = client.audio.speech.create(\n",
        "            model=\"tts-1\",  # Make sure you have access to the TTS model\n",
        "            voice=\"onyx\",  # Ensure the voice exists\n",
        "            input=test_text\n",
        "        )\n",
        "        response.stream_to_file(speech_file_path)\n",
        "        #print(f\"Audio saved to {speech_file_path}\")\n",
        "        return response\n",
        "    except Exception as e:\n",
        "        print(f\"Error generating audio: {e}\")\n",
        "        \n",
        "\"\"\"\n",
        "\n",
        "# Test AssemblyAI transcription\n",
        "def test_transcription(audio_file_path):\n",
        "    config = aai.TranscriptionConfig(speaker_labels=True)\n",
        "\n",
        "    try:\n",
        "        transcript = transcriber.transcribe(audio_file_path, config)\n",
        "\n",
        "        if transcript.status == aai.TranscriptStatus.completed:\n",
        "            print(\"Transcription:\", transcript.text)\n",
        "        else:\n",
        "            print(\"Transcription failed:\", transcript.error)\n",
        "    except Exception as e:\n",
        "        print(f\"Error transcribing audio: {e}\")\n",
        "\n",
        "\n",
        "# Run tests\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"Testing Chat functionality:\")\n",
        "    #test_chat()\n",
        "\n",
        "    print(\"\\nTesting OpenAI TTS:\")\n",
        "    test_tts()\n",
        "\n",
        "    print(\"\\nTesting AssemblyAI Transcription:\")\n",
        "    # Ensure you provide a valid audio file path here for testing transcription\n",
        "    test_audio_file_path = \"test.mp3\"  # Replace with actual path\n",
        "    test_transcription(test_audio_file_path)\n",
        " \"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "* Running on local URL:  http://127.0.0.1:7877\n",
            "* Running on public URL: https://b574543ee540055cab.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div><iframe src=\"https://b574543ee540055cab.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": []
          },
          "execution_count": 74,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/m9/2_1nwl_907v26kxr1jj5r__40000gn/T/ipykernel_44687/2774431761.py:32: DeprecationWarning: Due to a bug, this method doesn't actually stream the response content, `.with_streaming_response.method()` should be used instead\n",
            "  response.stream_to_file(speech_file_path)\n"
          ]
        }
      ],
      "source": [
        "import gradio as gr\n",
        "import openai\n",
        "from dotenv import load_dotenv\n",
        "import os\n",
        "\n",
        "# Load the environment variables (API key) from the .env file\n",
        "load_dotenv()\n",
        "\n",
        "# Set the OpenAI API key from environment variables\n",
        "openai.api_key = os.getenv('OPENAI_API_KEY')\n",
        "\n",
        "# Initial prompt for the AI system\n",
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": \"You are a Therapist, act as caring and understanding as possible\"}\n",
        "]\n",
        "\n",
        "# Test AssemblyAI transcription\n",
        "def transcribe(audio_file_path):\n",
        "    config = aai.TranscriptionConfig(speaker_labels=True, language_code=\"es\")\n",
        "\n",
        "\n",
        "\n",
        "    try:\n",
        "        transcript = transcriber.transcribe(audio_file_path, config)\n",
        "\n",
        "        if transcript.status == aai.TranscriptStatus.completed:\n",
        "            test_tts(transcript.text)\n",
        "            with open(\"test.mp3\", \"rb\") as audio_file:\n",
        "                audio_data = audio_file.read()\n",
        "            return audio_data\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"Error transcribing audio: {e}\")\n",
        "\n",
        "# Create the Gradio interface\n",
        "# Removed the 'source' parameter from gr.Audio\n",
        "ui = gr.Interface(fn=transcribe, inputs=gr.Audio(type=\"filepath\"), outputs=\"audio\")\n",
        "\n",
        "# Launch the interface and share it publicly\n",
        "ui.launch(share=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "flask",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
